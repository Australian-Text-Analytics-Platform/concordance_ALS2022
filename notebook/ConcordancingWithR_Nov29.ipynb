{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQPxWTAu7be3"
   },
   "source": [
    "\n",
    "\n",
    "# Concordancing using R\n",
    "\n",
    "This tutorial is based on the interactive Jupyter notebook accompanying the Language Technology and Data Analysis Laboratory (LADAL) tutorials on [text analysis and distant reading](https://ladal.edu.au/textanalysis.html) and on [concordancing](https://ladal.edu.au/kwics.html). This tutorial is aimed at beginners and intermediate users of R with the aim of showcasing how to use basic concordancing techniques on textual data using R.\n",
    "\n",
    "\n",
    "**Preparation and session set up**\n",
    "\n",
    "This tutorial is based on R. If you are new to R, you will find an introduction to it and more information on how to use R [here](https://ladal.edu.au/intror.html). For this tutorial, we need to activate certain *packages* from an R *library* so that the scripts shown below are executed without errors. If you want to run this notebook on your own computer, you will need to install a notebook server (e.g. [Anaconda](https://anaconda.org/)) and then install the R packages using the code in the first cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages\n",
    "install.packages('quanteda')\n",
    "install.packages('tidyverse')\n",
    "install.packages('tidytext')\n",
    "install.packages('tm')\n",
    "install.packages('flextable')\n",
    "install.packages('plyr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3166,
     "status": "ok",
     "timestamp": 1655867125790,
     "user": {
      "displayName": "Ben Foley",
      "userId": "01162525046993357054"
     },
     "user_tz": -600
    },
    "id": "XuSiAEUa7bfA",
    "outputId": "71d63337-e371-43d7-e91c-eb5429568801"
   },
   "outputs": [],
   "source": [
    "# set options\n",
    "options(stringsAsFactors = F)\n",
    "options(scipen = 999)\n",
    "options(max.print=1000)\n",
    "# load packages\n",
    "library(quanteda)\n",
    "library(tidyverse)\n",
    "library(tidytext)\n",
    "library(tm)\n",
    "library(flextable)\n",
    "library(plyr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading some data\n",
    "\n",
    "The data we will use today is a a part of the COrpus of Oz Early English (COOEE, Fritz 2007). COOEE is stratified in two ways; it has four temporal slices and four registers. We are using the material from the third time period (1851-1875) and for each register (Government English, Private Written, Public Written and Speech Based) the texts have been joined in a single file. The code reads all the txt files in the local directory - if you add any files .txt there and then rerun this cell, the additional files will be read and may cause problems. \n",
    "This cell, and most of those which follow, produce visible outputs. This is good practice as it allows us to see whether the code is producing the results which we expect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 2634,
     "status": "ok",
     "timestamp": 1655867544808,
     "user": {
      "displayName": "Ben Foley",
      "userId": "01162525046993357054"
     },
     "user_tz": -600
    },
    "id": "RtypNN68zXQO",
    "outputId": "d28d4f18-34d6-48da-f5c4-f8800e676a50"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "my_files <- list.files(pattern = \"\\\\.txt$\")\n",
    "# inspect\n",
    "my_files\n",
    "\n",
    "## read in data\n",
    "cooee <- lapply(my_files, readLines)\n",
    "\n",
    "# join tweets as text objects\n",
    "for (i in 1:length(cooee)) {\n",
    "    name <- substr(my_files[i], 1, nchar(my_files[i]) - 4)\n",
    "    assign(name, paste(as.data.frame(cooee[i])))\n",
    "    i = i + 1\n",
    "}\n",
    "#inspect\n",
    "substr(privateW, 1, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "\n",
    "In a first step, we load a corpus, convert everything to lower case, remove non-word symbols (including punctuation), and remove extra spaces. Compare the output from the previous cell to the results here.\n",
    "\n",
    "Part of the code here is a user-defined function. This is a way of modularising code - procedures which we expect to reuse can be split off and then called by other code as a complete procedure. The layout here is important: a function has to be read to memory before it can be called.\n",
    "\n",
    "At this point, you can choose which group of data you want to explore (governmentE,privateW, publicW, speechB - case is important!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "executionInfo": {
     "elapsed": 307,
     "status": "ok",
     "timestamp": 1655868110734,
     "user": {
      "displayName": "Ben Foley",
      "userId": "01162525046993357054"
     },
     "user_tz": -600
    },
    "id": "zJYtl_4T0riK",
    "outputId": "696396f3-0d22-499f-8791-1c26b35c55f8"
   },
   "outputs": [],
   "source": [
    "txtclean <- function(x){\n",
    "  x <- x %>%\n",
    "    iconv(to = \"UTF-8\") %>%\n",
    "    # all lower case\n",
    "    base::tolower() %>%\n",
    "    # remove non-word characters\n",
    "    stringr::str_replace_all(\"[^[:alpha:][:space:]]*\", \"\")  %>%\n",
    "    # remove punctuation\n",
    "    tm::removePunctuation() %>%\n",
    "    # remove superfluous white spaces\n",
    "    stringr::str_squish() %>%\n",
    "    paste(collapse = \" \")\n",
    "}\n",
    "\n",
    "# you can choose which register to look at by changing the argument passed to txtclean\n",
    "# (governmentE,privateW, publicW, speechB)\n",
    "kwic_data <- txtclean(publicW)\n",
    "substr(kwic_data, 1, 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordancing\n",
    "\n",
    "**Concordancing**, also known as **keyword-in-context (kwic)** analysis, is a very useful first step in exploring text data. A concordance is a list of all the occurrences of a type in the text, presented as a table where the keyword is centred and aligned and context on either side is also shown.\n",
    "\n",
    "The `quanteda` package includes a concordancing function which allows a number of parameters to be adjusted. We will explore some of these in the remainder of this session. The relevant function is called `kwic`, and minimally it requires us to specify a text to search in and a pattern to match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwic_results <- kwic(\n",
    "  # define text\n",
    "  kwic_data, \n",
    "  # define search pattern\n",
    "  pattern = \"australia\")\n",
    "if (nrow(kwic_results) > 10) {\n",
    "    kwic_results[1:10]\n",
    "} else {\n",
    "    kwic_results\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The warning message here indicates that `quanteda::kwic` is designed to work on tokenised data. As we will see, the results are exactly the same, but in order to avoid the warning messages, we will create a tokenised version of our text and use that as we go on.\n",
    "\n",
    "We can check how many tokens of the target there are by querying the number of rows in the results table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwic_token <- quanteda::tokens(kwic_data)\n",
    "kwic_results <- quanteda::kwic(\n",
    "  # define and tokenize text\n",
    "  kwic_token, \n",
    "  # define search pattern\n",
    "  pattern = \"australia\")\n",
    "\n",
    "nrow(kwic_results)\n",
    "if (nrow(kwic_results) > 10) {\n",
    "    kwic_results[1:10]\n",
    "} else {\n",
    "    kwic_results\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One parameter which we can manipulate with `kwic` is the amount of context we see around the keyword. The default is to provide five tokens before and five after the search term; our next concordance increases this to 10 items either side. What counts as a *token* for these purposes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwic_results_longer <- kwic(\n",
    "  # define text\n",
    "  kwic_token, \n",
    "  # define search pattern\n",
    "  pattern = \"australia\", \n",
    "  # define context window size\n",
    "  window = 10)\n",
    "if (nrow(kwic_results_longer) > 10) {\n",
    "    kwic_results_longer[1:10]\n",
    "} else {\n",
    "    kwic_results_longer\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concordancing - multiple words\n",
    "\n",
    "`kwic` can also generate a concordance for a phrase - we just have to tell the function that the pattern we are interested in **is** a phrase using the syntax `pattern = phrase(\"[Target Words]\")`. Again, we can also find out how many tokens of our target word(s) there are by checking how many rows there are in the output table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwic_phrase <- kwic(kwic_token, pattern = phrase(\"australia felix\"))\n",
    "nrow(kwic_phrase)\n",
    "if (nrow(kwic_phrase) > 10) {\n",
    "    kwic_phrase[1:10]\n",
    "} else {\n",
    "    kwic_phrase\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arranging concordances\n",
    "\n",
    "Often it is useful to sort the results of a concordance according to what is in the context. Doing this can tell us whether a word commonly co-occurs with other words. This is easy to do for the following context; we can sort the start of the `post` string alphabetically. Sorting by the preceding word is more complex. We leave it as an exercise for you to think about how this might be accomplished programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwic_ordered <- kwic(x = kwic_token, pattern = \"australia\") %>%\n",
    "  dplyr::arrange(post)\n",
    "if (nrow(kwic_ordered) > 10) {\n",
    "    kwic_ordered[1:10]\n",
    "} else {\n",
    "    kwic_ordered\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more sophisticated approach can include information about the frequency of the co-occurrence pattern and show us the most frequent result first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get frequency of word from frequency table\n",
    "get_freq <- function(word) {\n",
    "    freq <- subset(post_freq, post_freq$x == word)[['freq']]\n",
    "}\n",
    "\n",
    "# get concordance results\n",
    "kwic_result <- as.data.frame(kwic(kwic_token, pattern = 'australia'))\n",
    "# add column with first word following search term\n",
    "kwic_result$post_word <- str_remove_all(kwic_aus$post, \" .*\")\n",
    "# make table of frequencies of folowing words\n",
    "post_freq <- plyr::count(kwic_result$post_word)\n",
    "# add frequency to concordance table and sort results in descending order\n",
    "kwic_result$post_freq <- lapply(kwic_aus$post_word, get_freq)\n",
    "kwic_result[order(-unlist(kwic_aus$post_freq)),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results\n",
    "\n",
    "You can export the results of the various analyses we have looked at here. In each case, the results are some kind of table, and R lets you write these to a file easily using the `write.csv` command. Here is an example of exporting a concordance. This code saves a file in the scratch space, you can download it from there to your computer. Alternatively, you can specify a location on your computer to save the file - but you need to supply a complete path (and that should use / dividers not \\\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export results\n",
    "write.csv(kwic_results_longer, 'australia_prw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0H8qCYb7bf0"
   },
   "source": [
    "# Citation & Session Info \n",
    "\n",
    "Fritz, Clemens WA. 2007. *From English in Australia to Australian English: 1788-1900.* Peter Lang Frankfurt.\n",
    "\n",
    "Schweinberger, Martin. 2022. *Text Analysis and Distant Reading using R*. Brisbane: The University of Queensland. url: https://ladal.edu.au/textanalysis.html.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "domyS3Xa7bf1"
   },
   "outputs": [],
   "source": [
    "sessionInfo()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": "",
  "colab": {
   "collapsed_sections": [
    "Uj7NqlLn7bf1"
   ],
   "name": "Text Analysis in R.ipynb",
   "provenance": [
    {
     "file_id": "19vvWkulL2nOCV0RvAX4RBuHfLdNnPw1u",
     "timestamp": 1655600117085
    }
   ]
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
